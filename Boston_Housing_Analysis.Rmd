---
title: "Boston_Housing_Analysis"
author: "Sangjun Lim"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Packages
Load the required packages for the analysis.
```{r, message=F}
library(knitr)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(car)
```

## Data
Load the dataset into the R environment for analysis.
```{r, message = F}
boston <- read_csv("BostonHousing.csv")
```
### Data Cleaning
The square root transformation was applied to *CRIM* to address skewness, resulting in the creation of the variable *CRIM_SQRT.*
```{r}
boston <- boston %>%
  mutate(CRIM_SQRT = sqrt(CRIM))
```

Check each column to determine if any missing values are present.
```{r}
print(colSums(is.na(boston)))
```

If there are missing values, replace them with the mean of the respective variable.
```{r}
boston <- boston %>%
  mutate(across(everything(), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))
```

After addressing the missing values, the dataset is now free of any missing entries.
```{r}
print(colSums(is.na(boston)))
```

The dataset contains three duplicate rows.
```{r}
print(nrow(boston[duplicated(boston), ]))
```

Remove duplicated rows and check if they are successfully removed.
```{r}
# Remove duplicated rows
boston <- boston %>% 
  distinct()

# Check if duplicated rows are successfully removed
print(nrow(boston[duplicated(boston), ]))
```

### Descriptive Stats & Correlation Matrix
Develop a descriptive statistics table for all numerical variables, including the mean and standard deviation, followed by a correlation matrix. 
```{r, fig.align='center'}
# Create correlation matrix
cor_matrix <- cor(boston)

# Function to mask the upper triangle and replace NA with space
lower_triangle_with_space <- function(matrix) {
  matrix[upper.tri(matrix)] <- NA
  return(matrix)
}

# Apply the function to the correlation matrix
cor_matrix_lower <- lower_triangle_with_space(cor_matrix)

# Format the correlation matrix to 3 digits
cor_matrix_lower <- format(round(cor_matrix_lower, 3), nsmall = 3)

# Calculate mean and standard deviation for each column
means <- colMeans(boston)
sds <- apply(boston, 2, sd)

# Combine mean, standard deviation and correlation matrix (table format)
result_table <- cbind(
  Mean = format(round(means, 3), nsmall = 3),
  SD = format(round(sds, 3), nsmall = 3),
  cor_matrix_lower
)

# Print the table
kable(result_table)
```

## Multiple Linear Regression Model
Create histograms for each numerical variable, displaying their distributions in the Boston housing dataset.
```{r, fig.align='center'}
# Reshape the data to a long format
boston_long <- boston %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

# Histogram for each numerical variable in the dataset
ggplot(boston_long, aes(x = value)) +
  geom_histogram(bins = 10, fill = "skyblue", color = "black") +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(title = "Histograms of Numerical Variables in Boston Housing Dataset",
       x = "Value",
       y = "Frequency")
```

Check the linear relationship between MEDV and each independent variable.
```{r, fig.align='center', message=F}
boston_long <- boston %>%
  pivot_longer(cols = -MEDV, names_to = "variable", values_to = "value")

# Scatterplot of MEDV vs. Independent variables
ggplot(boston_long, aes(x = value, y = MEDV)) +
  geom_point(alpha = 0.8, size = 0.75) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(title = "Scatterplots of MEDV and each Independent Variable",
       x = "Value",
       y = "MEDV")
```

Create multiple linear regression model using MEDV as the dependent variable and the rest of variables as independent variables.
```{r}
mlr_model <- lm(MEDV ~ ., data = boston)
```

### Model Diagnostics
Check 4 assumptions of linear regression.
```{r}
par(mfrow = c(2, 2))
plot(mlr_model)
```

The plots test the following assumptions:

1.  **Residuals vs. Fitted**: Deviations from a horizontal line indicate non-linearity, suggesting the need for a different modeling approach.

2.  **Normal Q-Q Plot**: Deviations from the diagonal line indicate non-normality of the residuals, which may affect inference.

3.  **Scale-Location Plot**: A non-horizontal line or clustered points suggest heteroscedasticity, indicating that the variance of residuals is not constant.

Check VIF (Variance Inflation Factor) values for the predictors to assess multicollinearity.
```{r}
vif(mlr_model)
```

The VIF values indicate significant multicollinearity among the predictors. High VIF values suggest that some variables are highly correlated, which can compromise the stability of the regression coefficients. Notably, the VIF values for ***CRIM*** **(15.12)**, ***CRIM_SQRT*** **(37.68)**, ***RAD*** **(12.66)**, and ***TAX*** **(9.00)** are particularly concerning.

### Summary
```{r}
summary(mlr_model)
```

The model demonstrates a good fit, explaining approximately 73.37% of the variance in the variable *MEDV* with an adjusted R-squared of 0.7267. This indicates that the predictors included in the model are effective in explaining the variability in home prices in Boston.

The coefficients in the model exhibit varying levels of statistical significance. Key predictors like the *RM*, *NOX*, and *LSTAT* are highly significant, with p-values less than 0.001, indicating strong evidence that these variables meaningfully influence variable *MEDV*.

However, the model faces violations of assumptions related to non-linearity, non-normality of residuals, and multicollinearity, which should be addressed to enhance the reliability of the results. These issues may warrant consideration of alternative modeling methodologies to better capture the underlying relationships and improve overall model performance.

## Logistic Regression Model
A binary outcome variable named *MEDV_BINARY* was created to predict whether a home has a median value above the median (1) or not (0). Logistic regression was then employed to predict this outcome based on various features of the homes, including the square root of the per capita crime rate (*CRIM_SQRT*), the average number of rooms per dwelling (*RM*), the percentage of lower status residents (*LSTAT*), and the full-value property tax rate per $10,000 (*TAX*).
```{r}
boston <- boston %>%
  mutate(MEDV_BINARY = ifelse(MEDV > median(MEDV), 1, 0))

# Logistic regression model
logistic_model <- glm(MEDV_BINARY ~ CRIM_SQRT + RM + LSTAT + TAX, data = boston, family = binomial)
```

Check VIF values for the predictors to assess multicollinearity.
```{r}
vif(logistic_model)
```

None of the variables have serious multicollinearity issues, as all VIF values are below 5.

```{r}
summary(logistic_model)
```

### Coffeicient Interpretations

#### Intercept (B0)

The intercept represents the log-odds of the outcome (i.e., MEDV_BINARY = 1) when all predictors are zero. However, in this context, it lacks practical significance, as a value of zero for these predictors is not realistic.

#### CRIM_SQRT (B1 = 0.071597)

For each additional unit increase in CRIM_SQRT, the log-odds of being classified as a median value above the median increase increase by 0.071597.

The odds ratio for CRIM_SQRT can be calculated as <small>$e^{(0.071597)} = 1.07422235$</small>, meaning that for each unit increase in CRIM_SQRT, the odds of a home having a median value above the median increase by approximately 7.4%.

#### RM (B2 = 1.325798)

For each additional unit increase in RM, the log-odds of a home being classified as having a median value above the median increase by 1.325798. 

The odds ratio for RM can be calculated as <small>$e^{(1.325798)} \approx 3.76518878$</small>, meaning that for each unit increase in RM, the odds of a home having a median value above the median increase by approximately 276.5%.

#### LSTAT (B3 = -0.314980)

For each additional unit increase in LSTAT, the log-odds of a home being classified as having a median value above the median decrease by 0.314980.

The odds ratio for LSTAT can be calculated as <small>$e^{(-0.314980)} = 0.72980347$</small>, meaning that for each unit increase in LSTAT, the odds of a home having a median value above the median decrease by approximately 27%. 

#### TAX (B4 = -0.002249)

For each additional unit increase in TAX, the log-odds of a home being classified as having a median value above the median decrease by 0.002249. 

The odds ratio for TAX can be calculated as <small>$e^{(-0.002249)} = 0.997753527$</small>, meaning that for each unit increase in TAX, the odds of a home having a median value above the median decrease by approximately 0.2%. 

### Significance of the Predictors
**CRIM_SQRT**: The p-value for CRIM_SQRT is 0.683, indicating that it is not statistically significant at the 5% level, suggesting that the square root of the crime rate is not a meaningful predictor of whether a home has a median value above the median.

**RM**: The p-value for RM is <small>$5.86 × 10^{-5}$</small>, which is highly statistically significant, indicating that the average number of rooms is a strong predictor of whether a home has a median value above the median.

**LSTAT**: The p-value for LSTAT is <small>$2.04 × 10^{-15}$</small>, which is also highly statistically significant, indicating that the percentage of lower status residents is a very strong predictor of whether a home has a median value above the median.

**TAX**: The p-value for TAX is 0.102, which is not statistically significant at the 5% level, suggesting that the property tax rate (Full-value property tax rate per $10,000) is not a meaningful predictor of whether a home has a median value above the median.

### Summary
**CRIM_SQRT** is a weak predictor of whether a home has a median value above the median. Each additional unit increase in CRIM_SQRT results in a 7.4% increase in the odds of being classified as having a median value above the median; however, this effect is not statistically significant at the 5% level.

**RM** is a very strong predictor of home value. For each additional unit increase in RM, the odds of a home being classified as having a median value above the median increase by approximately 276.5%, indicating a highly significant relationship.

**LSTAT** is also a strong predictor. Each additional unit increase in LSTAT leads to a decrease of about 27% in the odds of a home being classified as having a median value above the median, demonstrating a significant negative relationship.

**TAX** is a weak predictor of home value. For each additional unit increase in TAX, the odds of a home being classified as having a median value above the median decrease by approximately 0.2%. Furthermore, this effect is not statistically significant at the 5% level.